{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8de7f7c-0bc0-4866-9c62-85812174229f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/26 18:08:27 WARN Utils: Your hostname, Mehdis-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.101 instead (on interface en0)\n",
      "25/10/26 18:08:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/26 18:08:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/26 18:08:40 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/10/26 18:08:42 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on test set: 0.8441\n",
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|    0|       0.0|[0.93402440734486...|\n",
      "|    0|       0.0|[0.99646799893433...|\n",
      "|    0|       0.0|[0.96521858609493...|\n",
      "|    0|       0.0|[0.97931070942687...|\n",
      "|    1|       0.0|[0.60724918287314...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# If not done already\n",
    "spark = SparkSession.builder.appName(\"HeartDisease\").getOrCreate()\n",
    "\n",
    "# Step 1: Load the file (already uploaded)\n",
    "columns = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "]\n",
    "\n",
    "file_p = \"/Users/mehdiamian/Desktop/Sohrab/heart+disease/processed.cleveland.data\"  # or \"cleveland.csv\" if you renamed\n",
    "\n",
    "df = spark.read.csv(file_p, inferSchema=True)\n",
    "df = df.toDF(*columns)\n",
    "\n",
    "# Step 2: Clean rows containing \"?\" â€” convert to null, drop\n",
    "for col_name in [\"ca\", \"thal\"]:\n",
    "    df = df.withColumn(col_name, col(col_name).cast(\"string\"))\n",
    "    df = df.filter(~(col(col_name) == \"?\"))\n",
    "\n",
    "# Convert cleaned columns to float\n",
    "df = df.withColumn(\"ca\", col(\"ca\").cast(\"float\"))\n",
    "df = df.withColumn(\"thal\", col(\"thal\").cast(\"float\"))\n",
    "\n",
    "# Convert target to binary (0 = no disease, >=1 = disease)\n",
    "df = df.withColumn(\"label\", (col(\"target\") > 0).cast(\"integer\"))\n",
    "\n",
    "# Step 3: Assemble features\n",
    "feature_cols = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "final_df = assembler.transform(df).select(\"features\", \"label\")\n",
    "\n",
    "# Step 4: Split data\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 5: Train model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "# Step 6: Predict & Evaluate\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"AUC on test set: {auc:.4f}\")\n",
    "\n",
    "# Optional: Show predictions\n",
    "predictions.select(\"label\", \"prediction\", \"probability\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278fd868-e009-41a5-b247-bd56d5e017b7",
   "metadata": {},
   "source": [
    "Running a more advanced classifier, random forest in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d693a1-f33a-4485-8a82-cb10d2969f04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (Random Forest) on test set: 0.8460\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "auc_rf = evaluator.evaluate(rf_predictions)\n",
    "print(f\"AUC (Random Forest) on test set: {auc_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b81561-3b4c-4017-91a2-fd19516eb9f8",
   "metadata": {},
   "source": [
    "Expanding the binary classification task to multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bcc85b1-a857-46fa-81f5-8d58a486bd96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7826\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "acc = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b463186f-c18b-4d98-8cb0-cfca020befc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/26 18:10:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "spark.range(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a303bf5f-0202-408e-a99d-c3a175659a24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.range(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b342ae7e-9d5f-4ae9-87be-c0e168ad8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/Heart%20Disease/heart-disease.csv\"\n",
    "# local_file = \"/tmp/heart.csv\"\n",
    "\n",
    "# urllib.request.urlretrieve(url, local_file)\n",
    "\n",
    "# # Read with Spark\n",
    "# df = spark.read.csv(local_file, header=True, inferSchema=True)\n",
    "# df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb8f4c1f-879e-4e84-85fe-08a7e70abdb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|63.0|1.0|1.0|   145.0|233.0|1.0|    2.0|  150.0|  0.0|    2.3|  3.0|0.0| 6.0|     0|\n",
      "|67.0|1.0|4.0|   160.0|286.0|0.0|    2.0|  108.0|  1.0|    1.5|  2.0|3.0| 3.0|     2|\n",
      "|67.0|1.0|4.0|   120.0|229.0|0.0|    2.0|  129.0|  1.0|    2.6|  2.0|2.0| 7.0|     1|\n",
      "|37.0|1.0|3.0|   130.0|250.0|0.0|    0.0|  187.0|  0.0|    3.5|  3.0|0.0| 3.0|     0|\n",
      "|41.0|0.0|2.0|   130.0|204.0|0.0|    2.0|  172.0|  0.0|    1.4|  1.0|0.0| 3.0|     0|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "]\n",
    "\n",
    "file_path = \"/Users/mehdiamian/Desktop/Sohrab/heart+disease/processed.cleveland.data\"  # or \"cleveland.csv\" if you renamed\n",
    "\n",
    "df = spark.read.csv(file_path, inferSchema=True)\n",
    "df = df.toDF(*columns)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ca72f-8745-4700-bd22-fbbac94737c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
